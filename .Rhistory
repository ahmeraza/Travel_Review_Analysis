install.packages("ggplot2")
library(ggplot2)
p <- ggplot(mtcars, aes(wt, mpg))
p + geom_point(aes(size = qsec, colour = factor(cyl)))
install.packages("tidyverse")
install.packages("tinytex")
library(dplyr)
library(magrittr)
print(mtcars)
library(magrittr)
library(dplyr)
print(mtcars)
a <- filter(mtcars, carb>2)
b <- group_by(a, cyl)
c <- summarise(b, avg_mpg=mean(mpg))
c <- summarise(b, avg_mpg=mean(mpg) )
view(c)
View(c)
d <- filter(c, avg_mpg > 15)
z <- filter(
summarise(
group_by(
filter(
mtcars, carb>2
)
,cyl
),
avg_mpg = mean(mpg)
),
avg_mpg > 15)
piped_df <- mtcars %>%
filter(carb>2) %>%
group_by(cyl) %>%
summarise(avg_mpg = mean(mpg)) %>%
filter(avg_mpg > 15)
my_txt <- c("I think that data is the new bacon",
"what is more. I think that text analytics is the new turkey bacon",
"very few people know how good Canadian bacon is",
"and even fewer people know how a beaver tail tastes like",
"Putin french fries are not so good")
#install.packages("xxxxxx")
library(dplyr)
mydf <- data.frame(line=1:5, text=my_txt)
print(my_txt)
View(mydf)
View(mydf)
library(tidytext)
#install.packages("tidytext")
library(tidytext)
#install.packages("tidyverse")
######################################################
######## Step3: tokenizing the mydf dataframe#########
######################################################
#install.packages("tidytext")
#install.packages("tidyverse")
library(tidytext)
######################################################
######## Step3: tokenizing the mydf dataframe#########
######################################################
#install.packages("tidytext")
#install.packages("tidyverse")
library(tidytext)
######################################################
######## Step3: tokenizing the mydf dataframe#########
######################################################
#install.packages("tidytext")
install.packages("tidytext")
library(tidytext)
library(tidyverse)
token_list <- mydf %>%
unnest_tokens(word, text)
#no punctutation, no upper case letters
print(token_list)
frequencies_tokens <- mydf %>%
unnest_tokens(word,text) %>%
count(word, sort=TRUE)
print(frequencies_tokens)
install.packages("stringr")
library(stringr)
library(stringr)
data(stop_words)
frequencies_tokens_nostop <- mydf %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>% #here's where we remove tokens
count(word, sort=TRUE)
print(frequencies_tokens_nostop)
library(ggplot2)
freq_hist <- mydf %>%
unnest_token(word,text) %>%
anti_join(stop_words) %>%
count(word, sort=TRUE) %>%
mutate(word=reorder(word, n)) %>%
ggplot(aes(word, n))+
geom_col()+
xlab(NULL)+
coord_flip()
freq_hist <- mydf %>%
unnest_tokens(word,text) %>%
anti_join(stop_words) %>%
count(word, sort=TRUE) %>%
mutate(word=reorder(word, n)) %>%
ggplot(aes(word, n))+
geom_col()+
xlab(NULL)+
coord_flip()
print(freq_hist)
